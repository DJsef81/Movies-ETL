{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0.3 \n",
    "\n",
    "# PURPOSE OF ANALYSIS\n",
    "\n",
    "# Amazing Prime Video \n",
    "\n",
    "# Algorithm which low budget movies being released will be popular, \n",
    "\n",
    "# buy the streaming rights at a bargain\n",
    "\n",
    "# Hackathon: where teams of analysts collaborate to work intensively on a project, using data to solve a problem. \n",
    "# Hackathons generally last several days and teams work around the clock on their projects.\n",
    "\n",
    "# This Hackathon: clean dataset of movie data, participants predict the popular movies. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How will Britta and I do this? \n",
    "\n",
    "# Britta has been tasked with creating datasets for hackathon. \n",
    "# - scrape of wikipedia of all movies realeased since 1990\n",
    "# - rating data from MovieLand website \n",
    "\n",
    "# For Britta, you'll extract scraped Wikipedia data stored as a JSON, and Kaggle data stored in CSVs.\n",
    "# Britta has determined that a SQL database is the best solution for sharing the data in the hackathon, so we'll be \n",
    "# loading our data into a PostgreSQL table. \n",
    "# Britta wants us to include ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL\n",
    "\n",
    "# Extract, Transform, Load \n",
    "\n",
    "# ETL process: extract the Wikipedia and Kaggle data from their respective files, transform the datasets by cleaning \n",
    "# them up and joining them together, and load the cleaned dataset into a SQL database.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Extract phase, data is pulled from external or internal sources, possibly disparate. \n",
    "\n",
    "# The sources could be flat files, scraped webpages in HTML or JavaScript Object Notation (JSON) format, SQL tables, \n",
    "# or even streams of sensor data. \n",
    "\n",
    "# The extracted data is held in a staging area in between the data sources and data targets.\n",
    "\n",
    "# For Britta, you'll extract scraped Wikipedia data stored as a JSON, and Kaggle data stored in CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After data is EXTRACTED, there are many transformations it may need to go through. \n",
    "\n",
    "# The data may need to be filtered, parsed, translated, sorted, interpolated, pivoted, summarized, aggregated, merged,\n",
    "# or more. \n",
    "\n",
    "# The goal is to create a consistent structure in the data. \n",
    "\n",
    "# Without a consistent structure in our data, it's almost impossible to perform any meaningful analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TRANSFORMATION phase can be accomplished with Python and Pandas, pure SQL, or specialized ETL tools like Apache \n",
    "# Airflow or Microsoft SQL Server Integrated Services (SSIS). \n",
    "\n",
    "# Python and Pandas are especially good for prototyping an ETL transformation because they provide flexibility and \n",
    "# interactivity (especially in a Jupyter Notebook), without enforcing any complicated frameworks. \n",
    "\n",
    "# We will use Python and Pandas to explore, document, and perform our data transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, after the data is transformed into a consistent structure, it's loaded into the data target. \n",
    "\n",
    "# The data target can be a relational database like PostgreSQL, a non-relational database like MongoDB that stores \n",
    "# individual documents, or a data warehouse like Amazon Redshift that optimizes performance specifically for analytics. \n",
    "\n",
    "# (We'll look at non-relational databases in more detail later.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Britta has determined that a SQL database is the best solution for sharing the data in the hackathon, so we'll be \n",
    "# loading our data into a PostgreSQL table. \n",
    "\n",
    "# SQL databases are often the targets of ETL processes, and because SQL is so ubiquitous, even databases that don't \n",
    "# use SQL often have SQL-like interfaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2.1 Extract the Wikipedia Movies JSON\n",
    "\n",
    "# Create a new GitHub repository named \"Movies-ETL.\" \n",
    "\n",
    "# Download the Wikipedia JSON file to your class folder.\n",
    "\n",
    "# Eventually, we'll want to create an automated pipeline, which Jupyter Notebooks aren't suited for. \n",
    "# But first, we'll need to do some exploratory data analysis, as Jupyter Notebooks are great for exploring data. \n",
    "# Then we can copy the code we've created to a Python script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the File\n",
    "\n",
    "# Now that our notebook is up and running and we've downloaded the Wikipedia movie data, we can start writing some code.\n",
    "\n",
    "# (create a new notebook to work on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
